{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "from word2vec_util import TextCorpusProcess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + '/data/images/' + 'img_prepared'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + '/drive/MyDrive/fb_marketplace/data/images/' + 'img_prepared'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pklname = data_path + '_train.pkl'\n",
    "train_data = joblib.load(train_pklname)\n",
    "text_data = train_data['desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "punct_dict = {\n",
    "    '.': '<PERIOD>',\n",
    "    ',': '<COMMA>',\n",
    "    '\"': '<QUOTATION_MARK>',\n",
    "    ';': '<SEMICOLON>',\n",
    "    '!': '<EXCLAMATION_MARK>',\n",
    "    '?': '<QUESTION_MARK>',\n",
    "    '(': '<QUESTION_MARK>',\n",
    "    ')': '<RIGHT_PAREN>',\n",
    "    '--': '<HYPHENS>',\n",
    "    '-': '<HYPHEN>',\n",
    "    ':': '<COLON>',\n",
    "    '|': '<PIPE>'\n",
    "}\n",
    "\n",
    "textCorpus = TextCorpusProcess(punct_dict=punct_dict, prod_descs=text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25184"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textCorpus.word_to_idx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "from skip_gram_neg import SkipGramNeg, NegativeSamplingLoss\n",
    "from word2vec_util import Word2VecDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "log_dir_path = '/content/drive/MyDrive/fb_marketplace/w2vruns'\n",
    "save_dir_path = '/content/drive/MyDrive/fb_marketplace/w2vsave/'\n",
    "try:\n",
    "    os.mkdir(log_dir_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(save_dir_path)\n",
    "except:\n",
    "    pass\n",
    "%tensorboard --logdir '{log_dir_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6465), started 0:08:17 ago. (Use '!kill 6465' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-43c3697fe5470b43\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-43c3697fe5470b43\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "current_path = os.getcwd()\n",
    "log_dir_path = current_path + '/runs/'\n",
    "save_dir_path = current_path + '/save/'\n",
    "try:\n",
    "    os.mkdir(log_dir_path)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(save_dir_path)\n",
    "except:\n",
    "    pass\n",
    "%tensorboard --logdir '{log_dir_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "writer = SummaryWriter(log_dir=log_dir_path)\n",
    "# get the noise distribution for negative sampling\n",
    "word_freqs = np.array(sorted(textCorpus.freq_dict.values(), reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(\n",
    "    unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "# hyperparams\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 16\n",
    "NEG_SAMPLE_SIZE = 5\n",
    "WINDOW_SIZE = 3\n",
    "EPOCHS = 20\n",
    "LR = 0.001\n",
    "\n",
    "# instantiating the model\n",
    "model = SkipGramNeg(\n",
    "    vocab_size=textCorpus.vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    noise_dist=noise_dist,\n",
    "    neg_sample_size=NEG_SAMPLE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# instantiating the dataloader\n",
    "dataloader = Word2VecDataloader(\n",
    "    prod_word_idx_lists=textCorpus.prod_word_idx_lists,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    window_size=WINDOW_SIZE,\n",
    ")\n",
    "\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 20\n",
      "Epoch training loss: 23.238672256469727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb#ch0000008?line=58'>59</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(input_vectors, output_vectors, noise_vectors)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb#ch0000008?line=59'>60</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb#ch0000008?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb#ch0000008?line=62'>63</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shbz/Dropbox/Workspace/Aicore/VSworkspace/fb_marketplace_reco/word2vec.ipynb#ch0000008?line=63'>64</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py:222\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    <a href='file:///home/shbz/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py?line=219'>220</a>\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='file:///home/shbz/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py?line=220'>221</a>\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[0;32m--> <a href='file:///home/shbz/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py?line=221'>222</a>\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[1;32m    <a href='file:///home/shbz/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py?line=222'>223</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/shbz/miniconda3/envs/fb_marketplace/lib/python3.9/site-packages/torch/optim/optimizer.py?line=223'>224</a>\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = EPOCHS\n",
    "# an arbitrary high value for best_epoch_loss\n",
    "best_epoch_loss = 100\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "for e in range(epochs):\n",
    "    batch_count = 0\n",
    "    running_loss = 0.0\n",
    "    # get input, target batches\n",
    "    for input_words, target_words in dataloader.generate_batch():\n",
    "        inputs = torch.LongTensor(input_words)\n",
    "        targets = torch.LongTensor(target_words)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # input, outpt, and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.generate_neg_samples(\n",
    "            # len(input_words), exclude_words=None, device=device)\n",
    "            len(input_words), exclude_words=target_words, device=device)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "        running_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "    exp_lr_scheduler.step()\n",
    "    epoch_loss = running_loss / batch_count\n",
    "    print(f'Epoch {e} / {epochs}')\n",
    "    print(f'Epoch training loss: {epoch_loss}')\n",
    "    writer.add_scalar(f'Training loss', epoch_loss, e)\n",
    "    writer.flush()\n",
    "\n",
    "    # deep copy the model\n",
    "    if epoch_loss < best_epoch_loss:\n",
    "        best_epoch_loss = epoch_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(best_model_wts, save_dir_path + 'best_word2vec_ebedding_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\n",
    "    save_dir_path + 'best_word2vec_ebedding_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 300\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(\n",
    "        textCorpus.idx_to_word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1a014fee8dab19e381f5c0a59544b00a460ed8d6f43556103265470604014b1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fb_marketplace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
